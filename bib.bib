@misc{Nickerson2014,
author = {Nickerson, Naomi},
title = {{precalculatedDecoded}},
url = {https://github.com/naominickerson/precalculatedDecoder},
urldate = {12/3/17},
year = {2014}
}
@article{Dennis2001,
abstract = {We analyze surface codes, the topological quantum error-correcting codes introduced by Kitaev. In these codes, qubits are arranged in a two-dimensional array on a surface of non-trivial topology, and encoded quantum operations are asso-ciated with nontrivial homology cycles of the surface. We formulate protocols for error recovery, and study the efficacy of these protocols. An order-disorder phase transition oc-curs in this system at a nonzero critical value of the error rate; if the error rate is below the critical value (the accuracy threshold), encoded information can be protected arbitrarily well in the limit of a large code block. This phase transition can be accurately modeled by a three-dimensional Z2 lattice gauge theory with quenched disorder. We estimate the ac-curacy threshold, assuming that all quantum gates are local, that qubits can be measured rapidly, and that polynomial-size classical computations can be executed instantaneously. We also devise a robust recovery procedure that does not re-quire measurement or fast classical processing; however for this procedure the quantum gates are local only if the qubits are arranged in four or more spatial dimensions. We discuss procedures for encoding, measurement, and performing fault-tolerant universal quantum computation with surface codes, and argue that these codes provide a promising framework for quantum computing architectures.},
archivePrefix = {arXiv},
arxivId = {arXiv:quant-ph/0110143v1},
author = {Dennis, Eric and Kitaev, Alexei and Landahl, Andrew and Preskill, John},
eprint = {0110143v1},
file = {:C$\backslash$:/Users/joshu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dennis et al. - 2001 - Topological quantum memory.pdf:pdf},
primaryClass = {arXiv:quant-ph},
title = {{Topological quantum memory *}},
url = {https://arxiv.org/pdf/quant-ph/0110143.pdf},
year = {2001}
}
@article{Nickerson,
abstract = {Many quantum technologies are now reaching a high level of maturity and control, and it is likely that the first demonstrations of suppression of naturally occurring quantum noise using small topological error correcting codes will soon be possible. In doing this it will be necessary to define clear achievable metrics to measure and compare the performance of real codes. Here we consider the smallest examples of several families of topological codes: surface codes, color codes, and the gauge color code, and determine the minimum requirements to demonstrate error suppression. We use an exact decoder to compare the performance of the codes under simple models of both physical error and measurement error, and determine regions of correctability in the physical parameters for each code.},
author = {Nickerson, Naomi H},
file = {:C$\backslash$:/Users/joshu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nickerson - Unknown - Error correcting power of small topological codes.pdf:pdf},
title = {{Error correcting power of small topological codes}},
url = {https://arxiv.org/pdf/1609.01753.pdf}
}
@article{Manin1980,
author = {Manin, YU. I},
journal = {Sov.Radio},
pages = {13--15},
title = {{Vychislimoe i nevychislimoe [Computable and Noncomputable]}},
year = {1980}
}
@article{Kolmogorov2009,
abstract = {We describe a new implementation of the Edmonds's algorithm for computing a perfect matching of minimum cost, to which we refer as Blossom V . A key feature of our implementation is a combination of two ideas that were shown to be effective for this problem: the “variable dual updates” approach of Cook and Rohe ({\{}INFORMS{\}} J Comput 11(2):138–148, 1999) and the use of priority queues. We achieve this by maintaining an auxiliary graph whose nodes correspond to alternating trees in the Edmonds's algorithm. While our use of priority queues does not improve the worst-case complexity, it appears to lead to an efficient technique. In the majority of our tests Blossom V outperformed previous implementations of Cook and Rohe ({\{}INFORMS{\}} J Comput 11(2):138–148, 1999) and Mehlhorn and Sch{\"{a}}fer (J Algorithmics Exp ({\{}JEA){\}} 7:4, 2002), sometimes by an order of magnitude. We also show that for large {\{}VLSI{\}} instances it is beneficial to update duals by solving a linear program, contrary to a conjecture by Cook and Rohe.},
author = {Kolmogorov, Vladimir},
doi = {10.1007/s12532-009-0002-8},
file = {:C$\backslash$:/Users/joshu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kolmogorov - 2009 - Blossom V A new implementation of a minimum cost perfect matching algorithm.pdf:pdf},
issn = {18672949},
journal = {Mathematical Programming Computation},
keywords = {Classification (2000) 68R10,Mathematics,Subject},
number = {1},
pages = {43--67},
title = {{Blossom V: A new implementation of a minimum cost perfect matching algorithm}},
volume = {1},
year = {2009}
}
@article{Wang,
abstract = {We study the AEJ random-plaquette Z 2 gauge model (RPGM) in three spatial dimensions, a three-dimensional analog of the two-dimensional AEJ random-bond Ising model (RBIM). The model is a pure Z 2 gauge theory in which randomly chosen plaquettes (occurring with concen-tration p) have couplings with the ''wrong sign'' so that magnetic flux is energetically favored on these plaquettes. Excitations of the model are one-dimensional ''flux tubes'' that terminate at ''magnetic monopoles'' located inside lattice cubes that contain an odd number of wrong-sign plaquettes. Electric confinement can be driven by thermal fluctuations of the flux tubes, by the quenched background of magnetic monopoles, or by a combination of the two. Like the RBIM, the RPGM has enhanced symmetry along a ''Nishimori line'' in the p–T plane (where T is the temperature). The critical concentration p c of wrong-sign plaquettes at the confine-ment-Higgs phase transition along the Nishimori line can be identified with the accuracy threshold for robust storage of quantum information using topological error-correcting codes: if qubit phase errors, qubit bit-flip errors, and errors in the measurement of local check oper-ators all occur at rates below p c , then encoded quantum information can be protected perfectly from damage in the limit of a large code block. Through Monte-Carlo simulations, we mea-sure p c0 , the critical concentration along the T ¼ 0 axis (a lower bound on p c), finding p c0 ¼ :0293 AE :0002. We also measure the critical concentration of antiferromagnetic bonds in the two-dimensional RBIM on the T ¼ 0 axis, finding p c0 ¼ :1031 AE :0001. Our value of p c0 is incompatible with the value of p c ¼ :1093 AE :0002 found in earlier numerical studies of the RBIM, in disagreement with the conjecture that the phase boundary of the RBIM is Annals of Physics 303 (2003) 31–58 PII: S 0 0 0 3 -4 9 1 6 (0 2) 0 0 0 1 9 -2 vertical (parallel to the T axis) below the Nishimori line. The model can be generalized to a rank-r antisymmetric tensor field in d dimensions, in the presence of quenched disorder.},
author = {Wang, Chenyang and Harrington, Jim and Preskill, John},
file = {:C$\backslash$:/Users/joshu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Harrington, Preskill - Unknown - Confinement-Higgs transition in a disordered gauge theory and the accuracy threshold for quant(2).pdf:pdf},
title = {{Confinement-Higgs transition in a disordered gauge theory and the accuracy threshold for quantum memory}},
url = {www.elsevier.com/locate/aop}
}
@article{Deutsch1985,
author = {Deutsch, S},
file = {:C$\backslash$:/Users/joshu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deutsch - 1985 - Quantum Theory, the Church-Turing Principal and the universal quantum computer.pdf:pdf},
journal = {Royl Society of London},
number = {A},
pages = {97--117},
title = {{Quantum Theory, the Church-Turing Principal and the universal quantum computer}},
volume = {400},
year = {1985}
}
@article{Shor1994,
abstract = {A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time by at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and which have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored.},
archivePrefix = {arXiv},
arxivId = {arXiv:quant-ph/9508027v2},
author = {Shor, Peter W and Shor, P W},
eprint = {9508027v2},
file = {:C$\backslash$:/Users/joshu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shor, Shor - 1994 - Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer.pdf:pdf},
journal = {AT{\&}T Research},
keywords = {03D10,11Y05,68Q10,81P10,Church's thesis,Fourier transforms AMS subject classifications,algorithmic number theory,discrete logarithms,foundations of quantum mechanics,prime factorization,quantum computers,spin systems},
pages = {20--22},
primaryClass = {arXiv:quant-ph},
publisher = {IEEE Computer Society Press},
title = {{Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer *}},
year = {1994}
}
@article{1997,
abstract = {The first part of the article ({\S}l-{\S}6) is devoted mainly to one goal, namely, to showing the computational capacity of a quantum computer using the problem of a stabilizer in the group Zk as an example. The discrete logarithm and decomposition into prime factors can be reduced to this problem. The formal definition of a quantum computer (more precisely, a quantum scheme) appears in {\S}4. All necessary information from computing theory and quantum mechanics is contained in {\S}2 and {\S}3. The reading of the second part can begin with the theory of one-to-one quantum codes ({\S}8.1, {\S}8.2 and {\S}9). This theory is sufficiently complete, simple and self- contained. However, in the present paper it is considered as a means of solving the perturbation problem, that is, constructing a quantum computer from unreliable (inaccurate, being subject to perturbations) elements. To this topic we devote {\S}7, {\S}8.3, {\S}10 and {\S}11. Interesting results are obtained in {\S}10.1 and {\S}11. The rest is a rather tedious technical preparation. (So far the problem in hand has been considered only at the physical level of rigour. The formal approach revealed one subtle point where a naive argument may lead to an error, see {\S}10.1.) The initial formulation of the problem is not rigorous and rests on an intuitive picture of the way a real-world computational device might work. This is formalized in {\S}7, but the final formulation of the problem (the so-called polynomial error correction system, see Definition 10.4) appears only in the middle of {\S}10.1. {\S}0.},
author = {Китаев, Алексей Юрьевич and Kitaev, Aleksei Yur'evich},
doi = {10.4213/rm892},
file = {:C$\backslash$:/Users/joshu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Китаев, Kitaev - 1997 - Quantum computations algorithms and error correction(2).pdf:pdf},
issn = {0042-1316},
journal = {Uspekhi Matematicheskikh Nauk},
number = {6},
pages = {53--112},
title = {{Quantum computations: algorithms and error correction}},
volume = {52},
year = {1997}
}
@article{Stace2010,
abstract = {Many proposals for quantum information processing are subject to detectable loss errors. In this paper, we give a detailed account of recent results in which we showed that topological quantum memories can simultaneously tolerate both loss errors and computational errors, with a graceful tradeoff between the threshold for each. We further discuss a number of subtleties that arise when implementing error correction on topological memories. We particularly focus on the role played by degeneracy in the matching algorithms, and present a systematic study its effects on thresholds. We also discuss some of the implications of degeneracy for estimating phase transition temperatures in the random bond Ising model.},
archivePrefix = {arXiv},
arxivId = {0912.1159},
author = {Stace, Thomas M and Barrett, Sean D},
doi = {10.1103/PhysRevA.81.022317},
eprint = {0912.1159},
file = {:C$\backslash$:/Users/joshu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stace, Barrett - 2010 - Error correction and degeneracy in surface codes suffering loss.pdf:pdf},
issn = {10502947},
journal = {Physical Review A - Atomic, Molecular, and Optical Physics},
number = {2},
pages = {1--10},
title = {{Error correction and degeneracy in surface codes suffering loss}},
volume = {81},
year = {2010}
}
